# ğŸ“Œ CalMate-20B-KO-LoRA

## 1. ê°œìš”
CalMate-20B-KO-LoRAëŠ” **gpt-oss-20b** ê¸°ë°˜ í•œêµ­ì–´ ì¼ì • íŒŒì„œ ëª¨ë¸ì…ë‹ˆë‹¤.  
LoRA(Low-Rank Adaptation) ê¸°ë²•ì„ í™œìš©í•´ **ê°œì¸ ë¹„ì„œìš© ì¼ì • JSON ì¶”ì¶œ** íƒœìŠ¤í¬ì— ë§ì¶° íŠœë‹í–ˆìŠµë‹ˆë‹¤.

---

## 2. ëª¨ë¸ ì •ë³´
- **Base Model**: [`unsloth/gpt-oss-20b-unsloth-bnb-4bit`](https://huggingface.co/unsloth/gpt-oss-20b-unsloth-bnb-4bit)  
- **LoRA Adapter**: [`Seonghaa/CalMate-20B-KO-LoRA`](https://huggingface.co/Seonghaa/CalMate-20B-KO-LoRA)  
- **Task**: í•œêµ­ì–´ ìì—°ì–´ â†’ ì¼ì • JSON ë³€í™˜  
- **ì¶œë ¥ ìŠ¤í‚¤ë§ˆ**
```json
{
  "title": "string",
  "start": "YYYY-MM-DD HH:MM",
  "end": "YYYY-MM-DD HH:MM",
  "location": "string",
  "priority": "high|normal|low"
}
```

---

## 3. ë°ì´í„°ì…‹
- í˜•ì‹: `test.jsonl`, `train.jsonl` (JSON Lines)
- ê° ë¼ì¸ì€ `{"user": "...", "gold": {...}}` í˜•ì‹
- `gold`ëŠ” ì •ë‹µ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì‘ì„±ë¨

---

## 4. í•™ìŠµ í™˜ê²½
- **Framework**: PyTorch + Transformers + PEFT(LoRA)
- **Quantization**: 4-bit (nf4, double quantization)
- **í™˜ê²½**
  - GPU: A100 / T4
  - Python 3.10
  - CUDA 12.x
  - Transformers 4.43+
  - bitsandbytes 0.43+

---

## 5. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ
```python
from unsloth import FastLanguageModel
from peft import LoraConfig

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gpt-oss-20b-unsloth-bnb-4bit",
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj"],
        task_type="CAUSAL_LM",
    )
)

# ì´í›„ í•™ìŠµ ì§„í–‰
```

---

## 6. í‰ê°€ ë°©ë²•
ëª¨ë¸ê³¼ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•´ `test.jsonl`ì— ëŒ€í•´ í‰ê°€:
```bash
python evaluate_calmate.py     --test ./test.jsonl     --adapter-id Seonghaa/CalMate-20B-KO-LoRA     --base-id unsloth/gpt-oss-20b-unsloth-bnb-4bit     --max-new 512     --temperature 0.2     --top-p 0.9     --batch-size 1
```

---

## 7. ì‚¬ìš© ì˜ˆì‹œ
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

tokenizer = AutoTokenizer.from_pretrained("unsloth/gpt-oss-20b-unsloth-bnb-4bit")
model = AutoModelForCausalLM.from_pretrained(
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
    device_map="auto",
    load_in_4bit=True
)
model = PeftModel.from_pretrained(model, "Seonghaa/CalMate-20B-KO-LoRA")

prompt = "ë‚´ì¼ ì˜¤í›„ 3ì‹œì— ê°•ë‚¨ì—­ì—ì„œ ë¯¼ìˆ˜ë‘ íšŒì˜"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## 8. ë¼ì´ì„ ìŠ¤
- Base Model ë° LoRA ëª¨ë‘ Apache-2.0 ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜
- ë°ì´í„°ì…‹ì€ ë‚´ë¶€ ì œì‘ ë°ì´í„° + ê³µê°œ ì¼ì • ì˜ˆì‹œ ë°ì´í„° í˜¼í•©
